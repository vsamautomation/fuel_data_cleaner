{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4571a258-2b09-4733-9702-00d87acafdda",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-11-04T07:18:05.3680547Z",
       "execution_start_time": "2025-11-04T07:17:43.2662009Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "472a7da4-88e0-40ac-9e1f-6ae3e605a2ed",
       "queued_time": "2025-11-04T07:17:27.0601563Z",
       "session_id": "78e0656b-e32a-4987-98b4-d04ec9c9a998",
       "session_start_time": "2025-11-04T07:17:27.0617631Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        3,
        4,
        5,
        6,
        7,
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, 78e0656b-e32a-4987-98b4-d04ec9c9a998, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fsspec-wrapper 0.1.15 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Warning: PySpark kernel has been restarted to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install semantic-link-labs for extended Fabric analytics\n",
    "%pip install -q -U semantic-link-labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c3693c-3b25-45e7-83e0-a66de5d88f3e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-11-04T07:18:11.9839373Z",
       "execution_start_time": "2025-11-04T07:18:08.5234297Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d9660dba-9da7-41ce-94d2-2cf50641dea0",
       "queued_time": "2025-11-04T07:17:27.0643324Z",
       "session_id": "78e0656b-e32a-4987-98b4-d04ec9c9a998",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 78e0656b-e32a-4987-98b4-d04ec9c9a998, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful. Spark session and Claude AI client initialized\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sempy_labs\n",
    "import sempy.fabric as fabric\n",
    "from sempy_labs.report import ReportWrapper\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from sempy.fabric import FabricRestClient\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Set, Any, Optional\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "fabric_client = FabricRestClient()\n",
    "print(\"‚úÖ All imports successful. Spark session and Claude AI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857440c4-f5f1-4f3a-a600-a961f08a3e65",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-11-04T07:18:16.1927263Z",
       "execution_start_time": "2025-11-04T07:18:15.9073814Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "0bb84b8d-bb55-4a8e-b326-6ae2bb9672fe",
       "queued_time": "2025-11-04T07:18:15.9062741Z",
       "session_id": "78e0656b-e32a-4987-98b4-d04ec9c9a998",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 78e0656b-e32a-4987-98b4-d04ec9c9a998, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Report metadata extraction function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UTILITY FUNCTIONS AND DATA STRUCTURES\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class DatasetInfo:\n",
    "    \"\"\"Data structure to hold comprehensive dataset information\"\"\"\n",
    "    ds_id: str\n",
    "    ds_name: str\n",
    "    ws_id: str\n",
    "    ws_name: str\n",
    "    dependencies_df: Optional[pd.DataFrame] = None\n",
    "    tables_df: Optional[pd.DataFrame] = None\n",
    "    expressions_df: Optional[pd.DataFrame] = None\n",
    "    relationships_df: Optional[pd.DataFrame] = None\n",
    "    measures_df: Optional[pd.DataFrame] = None\n",
    "    columns_df: Optional[pd.DataFrame] = None\n",
    "    datasource: Dict = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class ReportMetadata:\n",
    "    \"\"\"Data structure to hold Power BI report metadata analysis\"\"\"\n",
    "    report_id: str\n",
    "    report_name: str\n",
    "    workspace_id: str\n",
    "    workspace_name: str\n",
    "    dataset_id: str\n",
    "    report_format: str\n",
    "    extraction_method: str\n",
    "    tables: List[str]\n",
    "    columns: List[str]\n",
    "    measures: List[str]\n",
    "    visuals_count: int\n",
    "    filters_count: int\n",
    "    extraction_success: bool\n",
    "    error_message: str = \"\"\n",
    "\n",
    "print(\"‚úÖ Report metadata extraction function defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f998280-b03c-428f-affa-6f4f02c3d74a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-11-04T07:18:18.8360974Z",
       "execution_start_time": "2025-11-04T07:18:18.5538071Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "15225ad6-2569-4a7b-8ee6-b8766080d774",
       "queued_time": "2025-11-04T07:18:18.5517383Z",
       "session_id": "78e0656b-e32a-4987-98b4-d04ec9c9a998",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 78e0656b-e32a-4987-98b4-d04ec9c9a998, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class PowerBIMetadataExtractor:\n",
    "    \"\"\"Extracts columns, tables, and measures from Power BI report metadata\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tables = set()\n",
    "        self.columns = set()\n",
    "        self.measures = set()\n",
    "        self.visual_details = []\n",
    "        self.filter_details = []\n",
    "        \n",
    "    def extract_from_json_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract metadata from JSON data\"\"\"\n",
    "        self._reset()\n",
    "        \n",
    "        # Extract from sections\n",
    "        sections = data.get('sections', [])\n",
    "        \n",
    "        for section_idx, section in enumerate(sections):\n",
    "            section_name = section.get('displayName', f'Section_{section_idx}')\n",
    "            \n",
    "            # Extract from section-level filters\n",
    "            filters = section.get('filters', [])\n",
    "            if isinstance(filters, str):\n",
    "                filters = json.loads(filters)\n",
    "            self._extract_from_filters(filters, 'section', section_name)\n",
    "            \n",
    "            # Extract from visual containers\n",
    "            visual_containers = section.get('visualContainers', [])\n",
    "            self._extract_from_visual_containers(visual_containers, section_name)\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'tables': sorted(list(self.tables)),\n",
    "            'columns': sorted(list(self.columns)),\n",
    "            'measures': sorted(list(self.measures)),\n",
    "            'summary': {\n",
    "                'total_tables': len(self.tables),\n",
    "                'total_columns': len(self.columns),\n",
    "                'total_measures': len(self.measures)\n",
    "            },\n",
    "            'visual_details': self.visual_details,\n",
    "            'filter_details': self.filter_details\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _reset(self):\n",
    "        \"\"\"Reset all collections for new extraction\"\"\"\n",
    "        self.tables.clear()\n",
    "        self.columns.clear()\n",
    "        self.measures.clear()\n",
    "        self.visual_details.clear()\n",
    "        self.filter_details.clear()\n",
    "    \n",
    "    def _extract_from_visual_containers(self, visual_containers: List[Dict], section_name: str):\n",
    "        \"\"\"Extract from visualContainers array\"\"\"\n",
    "        for visual_idx, visual_container in enumerate(visual_containers):\n",
    "            visual_config = visual_container.get('config', {})\n",
    "            if isinstance(visual_config, str):\n",
    "                visual_config = json.loads(visual_config)\n",
    "            visual_name = visual_config.get('name', f'Visual_{visual_idx}')\n",
    "            \n",
    "            # Extract from visual-level filters\n",
    "            filters = visual_container.get('filters', [])\n",
    "            if isinstance(filters, str):\n",
    "                filters = json.loads(filters)\n",
    "            self._extract_from_filters(\n",
    "                filters, \n",
    "                'visual', \n",
    "                f\"{section_name}->{visual_name}\"\n",
    "            )\n",
    "            \n",
    "            # Extract from singleVisual\n",
    "            single_visual = visual_config.get('singleVisual', {})\n",
    "            if single_visual:\n",
    "                self._extract_from_single_visual(single_visual, section_name, visual_name)\n",
    "    \n",
    "    def _extract_from_single_visual(self, single_visual: Dict, section_name: str, visual_name: str):\n",
    "        \"\"\"Extract from singleVisual object\"\"\"\n",
    "        visual_type = single_visual.get('visualType', 'unknown')\n",
    "        \n",
    "        # Extract from projections\n",
    "        projections = single_visual.get('projections', {})\n",
    "        projection_refs = []\n",
    "        \n",
    "        for projection_type, projection_list in projections.items():\n",
    "            for proj in projection_list:\n",
    "                query_ref = proj.get('queryRef', '')\n",
    "                if query_ref:\n",
    "                    projection_refs.append(query_ref)\n",
    "                    self._parse_query_ref(query_ref)\n",
    "        \n",
    "        # Extract from prototypeQuery\n",
    "        prototype_query = single_visual.get('prototypeQuery', {})\n",
    "        self._extract_from_prototype_query(prototype_query)\n",
    "        \n",
    "        # Extract from objects (labels and other formatting with field references)\n",
    "        objects = single_visual.get('objects', {})\n",
    "        if objects:\n",
    "            self._extract_field_reference(objects)\n",
    "        \n",
    "        # Store visual details\n",
    "        self.visual_details.append({\n",
    "            'section': section_name,\n",
    "            'visual_name': visual_name,\n",
    "            'visual_type': visual_type,\n",
    "            'projection_refs': projection_refs,\n",
    "            'has_prototype_query': bool(prototype_query)\n",
    "        })\n",
    "    \n",
    "    def _extract_from_prototype_query(self, prototype_query: Dict):\n",
    "        \"\"\"Extract from prototypeQuery object\"\"\"\n",
    "        # Extract tables from 'From' clause\n",
    "        from_clause = prototype_query.get('From', [])\n",
    "        for from_item in from_clause:\n",
    "            entity = from_item.get('Entity', '')\n",
    "            if entity and self._is_actual_table_name(entity):\n",
    "                self.tables.add(entity)\n",
    "        \n",
    "        # Extract columns and measures from 'Select' clause using unified extractor\n",
    "        select_clause = prototype_query.get('Select', [])\n",
    "        for select_item in select_clause:\n",
    "            self._extract_field_reference(select_item)\n",
    "    \n",
    "    def _extract_from_filters(self, filters: List[Dict], filter_type: str, context: str):\n",
    "        \"\"\"Extract from filters array\"\"\"\n",
    "\n",
    "        for filter_idx, filter_obj in enumerate(filters):\n",
    "            filter_name = filter_obj.get('name', f'Filter_{filter_idx}')\n",
    "            \n",
    "            # Extract from expression\n",
    "            expression = filter_obj.get('expression', {})\n",
    "            self._extract_from_expression(expression)\n",
    "            \n",
    "            # Extract from filter object (nested structure)\n",
    "            filter_def = filter_obj.get('filter', {})\n",
    "            if filter_def:\n",
    "                # Extract tables from 'From' clause in filter\n",
    "                from_clause = filter_def.get('From', [])\n",
    "                for from_item in from_clause:\n",
    "                    entity = from_item.get('Entity', '')\n",
    "                    if entity:\n",
    "                        self.tables.add(entity)\n",
    "                \n",
    "                # Extract from 'Where' clause - might contain column references\n",
    "                where_clause = filter_def.get('Where', [])\n",
    "                for where_item in where_clause:\n",
    "                    self._extract_from_where_condition(where_item)\n",
    "            \n",
    "            # Store filter details\n",
    "            self.filter_details.append({\n",
    "                'filter_type': filter_type,\n",
    "                'context': context,\n",
    "                'filter_name': filter_name,\n",
    "                'has_expression': bool(expression),\n",
    "                'has_filter_def': bool(filter_def)\n",
    "            })\n",
    "    \n",
    "    def _extract_field_reference(self, item: Dict):\n",
    "        \"\"\"Unified field reference extractor for Columns and Measures.\n",
    "        \n",
    "        Works for both:\n",
    "        - prototypeQuery.Select[] items\n",
    "        - objects.labels[].properties nested structures\n",
    "        - Any nested structure with Column/Measure patterns\n",
    "        \"\"\"\n",
    "        if not isinstance(item, dict):\n",
    "            return\n",
    "        \n",
    "        # Get Name if available (from Select clause)\n",
    "        name = item.get('Name', '')\n",
    "        \n",
    "        # Extract Column reference\n",
    "        if 'Column' in item:\n",
    "            column_def = item['Column']\n",
    "            if isinstance(column_def, dict):\n",
    "                entity = self._get_entity_from_expression(column_def)\n",
    "                property_name = column_def.get('Property', '')\n",
    "                \n",
    "                # If we have Name, parse it for the table name\n",
    "                if name and '.' in name:\n",
    "                    table_name, field_name = name.split('.', 1)\n",
    "                    if self._is_actual_table_name(table_name):\n",
    "                        self.tables.add(table_name)\n",
    "                        self.columns.add(f\"'{table_name}'[{field_name}]\")\n",
    "                # Otherwise use Entity from SourceRef\n",
    "                elif entity and property_name and self._is_actual_table_name(entity):\n",
    "                    self.tables.add(entity)\n",
    "                    self.columns.add(f\"'{entity}'[{property_name}]\")\n",
    "        \n",
    "        # Extract Measure reference\n",
    "        elif 'Measure' in item:\n",
    "            measure_def = item['Measure']\n",
    "            if isinstance(measure_def, dict):\n",
    "                entity = self._get_entity_from_expression(measure_def)\n",
    "                property_name = measure_def.get('Property', '')\n",
    "                \n",
    "                # If we have Name, parse it for the table name\n",
    "                if name and '.' in name:\n",
    "                    table_name, field_name = name.split('.', 1)\n",
    "                    if self._is_actual_table_name(table_name):\n",
    "                        self.tables.add(table_name)\n",
    "                        self.measures.add(f\"'{table_name}'[{field_name}]\")\n",
    "                # Otherwise use Entity from SourceRef\n",
    "                elif entity and property_name and self._is_actual_table_name(entity):\n",
    "                    self.tables.add(entity)\n",
    "                    self.measures.add(f\"'{entity}'[{property_name}]\")\n",
    "        \n",
    "        # Recursively check nested structures (for objects.labels, etc.)\n",
    "        for value in item.values():\n",
    "            if isinstance(value, dict):\n",
    "                self._extract_field_reference(value)\n",
    "            elif isinstance(value, list):\n",
    "                for list_item in value:\n",
    "                    if isinstance(list_item, dict):\n",
    "                        self._extract_field_reference(list_item)\n",
    "    \n",
    "    def _get_entity_from_expression(self, field_def: Dict) -> str:\n",
    "        \"\"\"Extract entity/table name from Expression.SourceRef.\n",
    "        \n",
    "        Handles both:\n",
    "        - {\"Expression\": {\"SourceRef\": {\"Entity\": \"TableName\"}}}  # Actual table\n",
    "        - {\"Expression\": {\"SourceRef\": {\"Source\": \"t\"}}}          # Alias\n",
    "        \"\"\"\n",
    "        expression = field_def.get('Expression', {})\n",
    "        if isinstance(expression, dict):\n",
    "            source_ref = expression.get('SourceRef', {})\n",
    "            if isinstance(source_ref, dict):\n",
    "                # Prefer Entity over Source (Entity is actual table name)\n",
    "                return source_ref.get('Entity', source_ref.get('Source', ''))\n",
    "        return ''\n",
    "    \n",
    "    def _extract_from_expression(self, expression: Dict):\n",
    "        \"\"\"Extract from expression object\"\"\"\n",
    "        if 'Column' in expression:\n",
    "            # Extract table from SourceRef\n",
    "            column_expr = expression['Column']\n",
    "            source_ref = column_expr.get('Expression', {}).get('SourceRef', {})\n",
    "            entity = source_ref.get('Entity', '')\n",
    "            if entity:\n",
    "                self.tables.add(entity)\n",
    "            \n",
    "            # Extract column property\n",
    "            property_name = column_expr.get('Property', '')\n",
    "            if property_name and entity:\n",
    "                self.columns.add(f\"'{entity}'[{property_name}]\")\n",
    "    \n",
    "    def _extract_from_where_condition(self, where_item: Dict):\n",
    "        \"\"\"Extract from WHERE condition\"\"\"\n",
    "        condition = where_item.get('Condition', {})\n",
    "        if 'In' in condition:\n",
    "            expressions = condition['In'].get('Expressions', [])\n",
    "            for expr in expressions:\n",
    "                self._extract_from_expression(expr)\n",
    "    \n",
    "    def _is_actual_table_name(self, table_name: str) -> bool:\n",
    "        \"\"\"Check if table name is an actual table, not a query alias/prefix.\"\"\"\n",
    "        if not table_name or not isinstance(table_name, str):\n",
    "            return False\n",
    "        \n",
    "        # Filter out single character aliases (d, s, c, _, etc.)\n",
    "        if len(table_name) <= 1:\n",
    "            return False\n",
    "        \n",
    "        # Filter out common query aliases\n",
    "        query_aliases = {'subquery', 'temp', 'alias', 'src', 'tgt'}\n",
    "        if table_name.lower() in query_aliases:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _parse_query_ref(self, query_ref: str):\n",
    "        \"\"\"Parse queryRef format (e.g., 'table.column' or 'table.measure')\"\"\"\n",
    "        if '.' in query_ref:\n",
    "            table_name, field_name = query_ref.split('.', 1)\n",
    "            if self._is_actual_table_name(table_name):\n",
    "                self.tables.add(table_name)\n",
    "            # We'll determine if it's a column or measure from prototype query\n",
    "            # For now, just store the full reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fb886-82cd-4c73-aa67-0728e148ac08",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-11-04T07:41:06.6593417Z",
       "execution_start_time": "2025-11-04T07:41:06.3105434Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "bb32a703-905f-482e-8961-3ddba00bca05",
       "queued_time": "2025-11-04T07:41:06.3055049Z",
       "session_id": "78e0656b-e32a-4987-98b4-d04ec9c9a998",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 26,
       "statement_ids": [
        26
       ]
      },
      "text/plain": [
       "StatementMeta(, 78e0656b-e32a-4987-98b4-d04ec9c9a998, 26, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FabricWorkspaceAnalyzer:\n",
    "    \"\"\"Main analyzer class implementing the complete workflow\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.workspaces_df = pd.DataFrame()\n",
    "        self.datasets_df = pd.DataFrame()\n",
    "        self.reports_df = pd.DataFrame()\n",
    "        self.pbi_reports_df = pd.DataFrame()\n",
    "        self.all_dataset_info = {}\n",
    "        self.report_metadata_list = []\n",
    "        self.report_objects_used = []\n",
    "        self.error_log = []  # Store detailed errors for later display\n",
    "        \n",
    "    def sanitize_df_columns(self, df, extra_columns=False, ws_id=None, ds_id=None, ws_name=None, ds_name=None):\n",
    "        \"\"\"Replaces spaces in column names with underscore to prevent errors during Spark Dataframe Creation\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        df.columns = [\n",
    "            re.sub(r'\\W+', \"_\", col.strip().lower())\n",
    "            for col in df.columns\n",
    "        ]\n",
    "\n",
    "        if extra_columns:\n",
    "            df['workspace_id'] = ws_id\n",
    "            df['dataset_id'] = ds_id\n",
    "            df['workspace_name'] = ws_name\n",
    "            df['dataset_name'] = ds_name\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def save_to_lakehouse(self, df, table_name, description=\"\"):\n",
    "        \"\"\"Save DataFrame to lakehouse using Spark\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                print(f\"  ‚ö†Ô∏è Skipping empty DataFrame for table: {table_name}\")\n",
    "                return\n",
    "                \n",
    "            # Add analysis timestamp\n",
    "            df_with_timestamp = df.copy()\n",
    "            df_with_timestamp['analysis_date'] = datetime.now()\n",
    "            \n",
    "            # Clean any Series objects or empty Series columns that could cause Arrow issues\n",
    "            for col in df_with_timestamp.columns:\n",
    "                col_series = df_with_timestamp[col]\n",
    "                # Check if column contains Series objects instead of scalars\n",
    "                if col_series.dtype == 'object':\n",
    "                    # Check if any value is a Series\n",
    "                    if any(isinstance(val, pd.Series) for val in col_series if pd.notna(val)):\n",
    "                        # Extract scalar from Series or set to None\n",
    "                        df_with_timestamp[col] = col_series.apply(\n",
    "                            lambda x: x.iloc[0] if isinstance(x, pd.Series) and not x.empty else (None if isinstance(x, pd.Series) else x)\n",
    "                        )\n",
    "                    # Check if entire column is empty (no rows)\n",
    "                    elif len(col_series) == 0:\n",
    "                        df_with_timestamp[col] = None\n",
    "\n",
    "            # Convert to Spark DataFrame and save\n",
    "            spark_df = spark.createDataFrame(df_with_timestamp)\n",
    "            spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "            \n",
    "            print(f\"  ‚úÖ Saved {len(df)} records to '{table_name}' table\")\n",
    "            if description:\n",
    "                print(f\"     üìù {description}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error saving to {table_name}: {str(e)}\")\n",
    "    def _extract_meaningful_error(self, error_msg):\n",
    "        \"\"\"\n",
    "        Extract the meaningful error message from exceptions.\n",
    "        Removes technical details like stack traces, activity IDs, and timestamps.\n",
    "        \"\"\"\n",
    "        lines = error_msg.split('\\n')\n",
    "        \n",
    "        # First line is usually the initial error\n",
    "        brief_error = lines[0].strip()\n",
    "        \n",
    "        # Look for the actual error message after \"Caused by\" or similar patterns\n",
    "        for i, line in enumerate(lines):\n",
    "            line_stripped = line.strip()\n",
    "            \n",
    "            # Look for patterns that indicate the meaningful error\n",
    "            if 'Caused by' in line_stripped and i + 1 < len(lines):\n",
    "                next_line = lines[i + 1].strip()\n",
    "                # Skip if next line is empty or starts with technical details\n",
    "                if next_line and not next_line.startswith('Technical Details:'):\n",
    "                    return next_line\n",
    "            \n",
    "            # Stop at technical details section\n",
    "            if 'Technical Details:' in line_stripped:\n",
    "                break\n",
    "            \n",
    "            # Stop at stack traces\n",
    "            if line_stripped.startswith('at '):\n",
    "                break\n",
    "        \n",
    "        # Return the first line if nothing better found\n",
    "        return brief_error\n",
    "\n",
    "    \n",
    "    def get_workspaces(self):\n",
    "        \"\"\"Step 1: Get Workspaces\"\"\"\n",
    "        print(\"üîç STEP 1: Discovering workspaces...\")\n",
    "        \n",
    "        self.workspaces_df = fabric.list_workspaces()\n",
    "        self.workspaces_df = self.sanitize_df_columns(self.workspaces_df)\n",
    "        self.workspaces_df = self.workspaces_df[['id', 'name', 'type']]\n",
    "        \n",
    "        print(f\"  ‚úÖ Found {len(self.workspaces_df)} workspaces\")\n",
    "        return self.workspaces_df\n",
    "    \n",
    "    def get_datasets_and_reports(self):\n",
    "        \"\"\"Step 2: Get Datasets and Reports in parallel\"\"\"\n",
    "        print(\"\\nüîç STEP 2: Getting datasets and reports...\")\n",
    "        \n",
    "        datasets_all, reports_all = [], []\n",
    "        \n",
    "        for _, ws in self.workspaces_df.iterrows():\n",
    "            ws_id = ws['id']\n",
    "            ws_name = ws['name']\n",
    "            ws_type = ws['type']\n",
    "            \n",
    "            if ws_type == \"AdminInsights\":\n",
    "                continue\n",
    "                \n",
    "            print(f\"  üì¶ Scanning workspace: {ws_name}\")\n",
    "            \n",
    "            # Get Datasets\n",
    "            try:\n",
    "                ds = fabric.list_datasets(workspace=ws_id)\n",
    "                if not ds.empty:\n",
    "                    ds['workspace_id'] = ws_id\n",
    "                    ds['workspace_name'] = ws_name\n",
    "                    datasets_all.append(ds)\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Datasets error in {ws_name}: {e}\")\n",
    "            \n",
    "            # Get Reports\n",
    "            try:\n",
    "                rep = fabric.list_reports(workspace=ws_id)\n",
    "                if not rep.empty:\n",
    "                    rep['workspace_id'] = ws_id\n",
    "                    rep['workspace_name'] = ws_name\n",
    "                    reports_all.append(rep)\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Reports error in {ws_name}: {e}\")\n",
    "        \n",
    "        # Combine results\n",
    "        self.datasets_df = self.sanitize_df_columns(pd.concat(datasets_all, ignore_index=True) if datasets_all else pd.DataFrame())\n",
    "        self.reports_df = self.sanitize_df_columns(pd.concat(reports_all, ignore_index=True) if reports_all else pd.DataFrame())\n",
    "        \n",
    "        # Filter PowerBI reports\n",
    "        if not self.reports_df.empty and \"report_type\" in self.reports_df.columns:\n",
    "            self.pbi_reports_df = self.reports_df[self.reports_df[\"report_type\"] == \"PowerBIReport\"].copy()\n",
    "        else:\n",
    "            self.pbi_reports_df = self.reports_df\n",
    "        \n",
    "        print(f\"  ‚úÖ Found {len(self.datasets_df)} datasets and {len(self.reports_df)} reports ({len(self.pbi_reports_df)} PowerBI reports)\")\n",
    "        return self.datasets_df, self.reports_df\n",
    "    \n",
    "    def process_all_datasets(self):\n",
    "        \"\"\"Step 3: Process all datasets and aggregate all objects (tables, columns, measures, dependencies, datasources)\"\"\"\n",
    "        print(\"\\nüîç STEP 3: Processing all datasets and aggregating objects...\")\n",
    "        \n",
    "        all_columns_list = []\n",
    "        all_tables_list = []\n",
    "        all_measures_list = []\n",
    "        all_dependencies_list = []\n",
    "        all_relationships_list = []\n",
    "        all_expressions_list = []\n",
    "        all_datasources_list = []\n",
    "        \n",
    "        for _, ds_row in self.datasets_df.iterrows():\n",
    "            ds_id = ds_row['dataset_id']\n",
    "            ds_name = ds_row['dataset_name']\n",
    "            ws_id = ds_row['workspace_id']\n",
    "            ws_name = ds_row['workspace_name']\n",
    "            \n",
    "            print(f\"  üìä Processing dataset: {ds_name}\")\n",
    "            \n",
    "            # Collect comprehensive dataset info\n",
    "            dataset_info = self.collect_dataset_info(ds_id, ds_name, ws_id, ws_name)\n",
    "            self.all_dataset_info[ds_id] = dataset_info\n",
    "            \n",
    "            # Aggregate columns\n",
    "            if dataset_info.columns_df is not None and not dataset_info.columns_df.empty:\n",
    "                all_columns_list.append(dataset_info.columns_df)\n",
    "            \n",
    "            # Aggregate tables\n",
    "            if dataset_info.tables_df is not None and not dataset_info.tables_df.empty:\n",
    "                all_tables_list.append(dataset_info.tables_df)\n",
    "            \n",
    "            #Aggregate datasources\n",
    "            if dataset_info.datasource and isinstance(dataset_info.datasource, dict) and len(dataset_info.datasource) > 0:\n",
    "                all_datasources_list.append(dataset_info.datasource)\n",
    "\n",
    "            # Aggregate measures\n",
    "            if dataset_info.measures_df is not None and not dataset_info.measures_df.empty:\n",
    "                # Add additional context that might not be in the measures_df\n",
    "                measures_with_context = dataset_info.measures_df.copy()\n",
    "                if 'dataset_id' not in measures_with_context.columns:\n",
    "                    measures_with_context['dataset_id'] = ds_id\n",
    "                if 'dataset_name' not in measures_with_context.columns:\n",
    "                    measures_with_context['dataset_name'] = dataset_info.ds_name\n",
    "                if 'workspace_id' not in measures_with_context.columns:\n",
    "                    measures_with_context['workspace_id'] = dataset_info.ws_id\n",
    "                if 'workspace_name' not in measures_with_context.columns:\n",
    "                    measures_with_context['workspace_name'] = dataset_info.ws_name\n",
    "                all_measures_list.append(measures_with_context)\n",
    "            \n",
    "            # Aggregate dependencies\n",
    "            if dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
    "                all_dependencies_list.append(dataset_info.dependencies_df)\n",
    "            \n",
    "            #Aggregate expressions\n",
    "            if dataset_info.expressions_df is not None and not dataset_info.expressions_df.empty:\n",
    "                all_expressions_list.append(dataset_info.expressions_df)\n",
    "\n",
    "            # Aggregate relationships\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                relationships_with_context = dataset_info.relationships_df.copy()\n",
    "                relationships_with_context['dataset_id'] = ds_id\n",
    "                relationships_with_context['dataset_name'] = dataset_info.ds_name\n",
    "                relationships_with_context['workspace_id'] = dataset_info.ws_id\n",
    "                relationships_with_context['workspace_name'] = dataset_info.ws_name\n",
    "                all_relationships_list.append(relationships_with_context)\n",
    "        \n",
    "        # Combine all aggregated data\n",
    "        all_columns_df = pd.concat(all_columns_list, ignore_index=True) if all_columns_list else pd.DataFrame()\n",
    "        all_tables_df = pd.concat(all_tables_list, ignore_index=True) if all_tables_list else pd.DataFrame()\n",
    "        all_measures_df = pd.concat(all_measures_list, ignore_index=True) if all_measures_list else pd.DataFrame()\n",
    "        all_dependencies_df = pd.concat(all_dependencies_list, ignore_index=True) if all_dependencies_list else pd.DataFrame()\n",
    "        all_relationships_df = pd.concat(all_relationships_list, ignore_index=True) if all_relationships_list else pd.DataFrame()\n",
    "        all_expressions_df = pd.concat(all_expressions_list, ignore_index= True) if all_expressions_list else pd.DataFrame()\n",
    "        all_datasources_df = pd.DataFrame(all_datasources_list) if all_datasources_list else pd.DataFrame()\n",
    "        \n",
    "        print(f\"  ‚úÖ Processed {len(self.all_dataset_info)} datasets\")\n",
    "        print(f\"    üìã Aggregated: {len(all_columns_df)} columns, {len(all_tables_df)} tables, {len(all_measures_df)} measures\")\n",
    "        print(f\"    üîó Aggregated: {len(all_dependencies_df)} dependencies, {len(all_relationships_df)} relationships, {len(all_expressions_df)} Expressions\")\n",
    "        \n",
    "        return all_columns_df, all_tables_df, all_measures_df, all_dependencies_df, all_relationships_df, all_expressions_df, all_datasources_df\n",
    "    \n",
    "    def get_reports_metadata(self):\n",
    "        \"\"\"Step 4: Get Reports metadata (what objects they use)\"\"\"\n",
    "        print(\"\\nüîç STEP 4: Extracting report metadata...\")\n",
    "        \n",
    "        if self.pbi_reports_df.empty:\n",
    "            print(\"  ‚ö†Ô∏è No PowerBI reports found\")\n",
    "            return []\n",
    "        \n",
    "        for idx, report_row in self.pbi_reports_df.iterrows():\n",
    "            report_id = report_row.get('id', '')\n",
    "            report_name = report_row.get('name', f'Report_{idx}')\n",
    "            workspace_id = report_row.get('workspace_id', '')\n",
    "            workspace_name = report_row.get('workspace_name', '')\n",
    "            dataset_id = report_row.get('dataset_id', '')\n",
    "            \n",
    "            print(f\"  üìä Processing report {idx+1}/{len(self.pbi_reports_df)+1}: {report_name}\")\n",
    "            \n",
    "            # Extract metadata\n",
    "            report_metadata = self.extract_report_metadata(\n",
    "                report_id, report_name, workspace_id, workspace_name, dataset_id\n",
    "            )\n",
    "            \n",
    "            self.report_metadata_list.append(report_metadata)\n",
    "            \n",
    "            # Create detailed records for each object used by this report\n",
    "            if report_metadata.extraction_success:\n",
    "                # Add table records\n",
    "                for table in report_metadata.tables:\n",
    "                    self.report_objects_used.append({\n",
    "                        'report_id': report_id,\n",
    "                        'report_name': report_name,\n",
    "                        'workspace_id': workspace_id,\n",
    "                        'workspace_name': workspace_name,\n",
    "                        'dataset_id': dataset_id,\n",
    "                        'object_type': 'Table',\n",
    "                        'object_name': table,\n",
    "                        'full_reference': table,\n",
    "                        'extraction_method': report_metadata.extraction_method\n",
    "                    })\n",
    "                \n",
    "                # Add column records\n",
    "                for column in report_metadata.columns:\n",
    "                    table_name = column.split(\"'\")[1]\n",
    "                    column_name = column.split(\"'\")[2].strip(\"[]\")\n",
    "                    self.report_objects_used.append({\n",
    "                        'report_id': report_id,\n",
    "                        'report_name': report_name,\n",
    "                        'workspace_id': workspace_id,\n",
    "                        'workspace_name': workspace_name,\n",
    "                        'dataset_id': dataset_id,\n",
    "                        'object_type': 'Column',\n",
    "                        'object_name': column_name,\n",
    "                        'full_reference': column,\n",
    "                        'table_name': table_name,\n",
    "                        'extraction_method': report_metadata.extraction_method\n",
    "                    })\n",
    "                \n",
    "                # Add measure records\n",
    "                for measure in report_metadata.measures:\n",
    "                    table_name = measure.split(\"'\")[1]\n",
    "                    measure_name = measure.split(\"'\")[2].strip(\"[]\")\n",
    "                    self.report_objects_used.append({\n",
    "                        'report_id': report_id,\n",
    "                        'report_name': report_name,\n",
    "                        'workspace_id': workspace_id,\n",
    "                        'workspace_name': workspace_name,\n",
    "                        'dataset_id': dataset_id,\n",
    "                        'object_type': 'Measure',\n",
    "                        'object_name': measure_name,\n",
    "                        'full_reference': measure,\n",
    "                        'table_name': table_name,\n",
    "                        'extraction_method': report_metadata.extraction_method\n",
    "                    })\n",
    "        \n",
    "        print(f\"  ‚úÖ Processed {len(self.report_metadata_list)+1} reports, extracted {len(self.report_objects_used)} object references\")\n",
    "        return self.report_metadata_list\n",
    "    \n",
    "    def check_dependencies(self, all_columns_df, all_tables_df, all_measures_df):\n",
    "        \"\"\"Step 5: Check for dependencies between objects\"\"\"\n",
    "        print(\"\\nüîç STEP 5: Checking for dependencies...\")\n",
    "        \n",
    "        # Convert report objects to DataFrame for easier analysis\n",
    "        report_objects_df = pd.DataFrame(self.report_objects_used) if self.report_objects_used else pd.DataFrame()\n",
    "        \n",
    "        # Get all used objects from reports\n",
    "        used_tables = set()\n",
    "        used_columns = set()\n",
    "        used_measures = set()\n",
    "        \n",
    "        if not report_objects_df.empty:\n",
    "            used_tables.update(report_objects_df[report_objects_df['object_type'] == 'Table']['full_reference'].tolist())\n",
    "            used_columns.update(report_objects_df[report_objects_df['object_type'] == 'Column']['full_reference'].tolist())\n",
    "            used_measures.update(report_objects_df[report_objects_df['object_type'] == 'Measure']['full_reference'].tolist())\n",
    "        \n",
    "        print(f\"  üìã Initial objects from reports: {len(used_tables)} tables, {len(used_columns)} columns, {len(used_measures)} measures\")\n",
    "        \n",
    "        # Check for dependencies within datasets (relationships and transitive dependencies)\n",
    "        for ds_id, dataset_info in self.all_dataset_info.items():\n",
    "            # Check relationships - columns used in relationships are required\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                for _, rel in dataset_info.relationships_df.iterrows():\n",
    "                    if 'qualified_from' in rel:\n",
    "                        used_columns.add(rel['qualified_from'])\n",
    "                    if 'qualified_to' in rel:\n",
    "                        used_columns.add(rel['qualified_to'])\n",
    "        \n",
    "        print(f\"  üîó After adding relationship columns: {len(used_columns)} columns\")\n",
    "        \n",
    "        # Transitive dependency resolution: find what the used objects depend on\n",
    "        # Keep iterating until no new dependencies are found\n",
    "        iteration = 0\n",
    "        max_iterations = 10  # Prevent infinite loops\n",
    "        \n",
    "        while iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            initial_tables_count = len(used_tables)\n",
    "            initial_columns_count = len(used_columns)\n",
    "            initial_measures_count = len(used_measures)\n",
    "            \n",
    "            print(f\"  üîÑ Dependency resolution iteration {iteration}...\")\n",
    "            \n",
    "            # Check dependencies for all used objects\n",
    "            for ds_id, dataset_info in self.all_dataset_info.items():\n",
    "                if dataset_info.dependencies_df is None or dataset_info.dependencies_df.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Iterate through each dependency row\n",
    "                for _, dep in dataset_info.dependencies_df.iterrows():\n",
    "                    # Get the full_object_name (the object that has the dependency)\n",
    "                    full_object_name = dep.get('full_object_name', '')\n",
    "                    \n",
    "                    # Check if this object is in our used sets\n",
    "                    if full_object_name in used_columns or full_object_name in used_measures:\n",
    "                        # This object is used, so we need to mark its dependencies as used too\n",
    "                        ref_object_type = dep.get('referenced_object_type', '')\n",
    "                        referenced_full_object_name = dep.get('referenced_full_object_name', '')\n",
    "                        \n",
    "                        if ref_object_type == 'Table':\n",
    "                            # The used object depends on a table\n",
    "                            table_name = dep.get('referenced_table', '')\n",
    "                            if table_name:\n",
    "                                used_tables.add(table_name)\n",
    "                        \n",
    "                        elif ref_object_type == 'Column':\n",
    "                            # The used object depends on a column\n",
    "                            if referenced_full_object_name:\n",
    "                                used_columns.add(referenced_full_object_name)\n",
    "                        \n",
    "                        elif ref_object_type == 'Measure':\n",
    "                            # The used object depends on a measure\n",
    "                            if referenced_full_object_name:\n",
    "                                used_measures.add(referenced_full_object_name)\n",
    "            \n",
    "            # Check if we found any new dependencies\n",
    "            new_tables = len(used_tables) - initial_tables_count\n",
    "            new_columns = len(used_columns) - initial_columns_count\n",
    "            new_measures = len(used_measures) - initial_measures_count\n",
    "            \n",
    "            print(f\"    ‚ûï Added: {new_tables} tables, {new_columns} columns, {new_measures} measures\")\n",
    "            \n",
    "            # If no new dependencies were found, we're done\n",
    "            if new_tables == 0 and new_columns == 0 and new_measures == 0:\n",
    "                print(f\"  ‚úÖ Dependency resolution converged after {iteration} iteration(s)\")\n",
    "                break\n",
    "        \n",
    "        print(f\"  ‚úÖ Final dependencies: {len(used_tables)} tables, {len(used_columns)} columns, {len(used_measures)} measures\")\n",
    "        \n",
    "        # Display detailed errors if any\n",
    "        if self.error_log:\n",
    "            print(f\"\\n‚ö†Ô∏è Detailed Error Log ({len(self.error_log)} errors):\")\n",
    "            for idx, error_entry in enumerate(self.error_log, 1):\n",
    "                print(f\"\\nError #{idx}:\")\n",
    "                print(f\"  Dataset: {error_entry['dataset']}\")\n",
    "                print(f\"  Operation: {error_entry['operation']}\")\n",
    "                print(f\"  Details: {error_entry['error']}\")\n",
    "        \n",
    "        return {\n",
    "            'used_tables': used_tables,\n",
    "            'used_columns': used_columns,\n",
    "            'used_measures': used_measures,\n",
    "            'report_objects_df': report_objects_df\n",
    "        }\n",
    "    \n",
    "    def filter_results(self, all_columns_df, all_tables_df, all_measures_df, dependencies):\n",
    "        \"\"\"Step 6: Filter results to identify used vs unused objects\"\"\"\n",
    "        print(\"\\nüîç STEP 6: Filtering results to identify used vs unused objects...\")\n",
    "        \n",
    "        used_tables = dependencies['used_tables']\n",
    "        used_columns = dependencies['used_columns']\n",
    "        used_measures = dependencies['used_measures']\n",
    "        \n",
    "        # Filter columns\n",
    "        if not all_columns_df.empty:\n",
    "            if 'qualified_name' in all_columns_df.columns:\n",
    "                all_columns_df['is_used'] = all_columns_df['qualified_name'].isin(used_columns)\n",
    "            else:\n",
    "                # Create qualified name if it doesn't exist\n",
    "                all_columns_df['qualified_name'] = \"'\" + all_columns_df['table_name'] + \"'[\" + all_columns_df['column_name'] + ']'\n",
    "                all_columns_df['is_used'] = all_columns_df['qualified_name'].isin(used_columns)\n",
    "            \n",
    "            used_columns_df = all_columns_df[all_columns_df['is_used'] == True].copy()\n",
    "            unused_columns_df = all_columns_df[all_columns_df['is_used'] == False].copy()\n",
    "        else:\n",
    "            used_columns_df = pd.DataFrame()\n",
    "            unused_columns_df = pd.DataFrame()\n",
    "        \n",
    "        # Filter tables\n",
    "        if not all_tables_df.empty:\n",
    "            all_tables_df['is_used'] = all_tables_df['name'].isin(used_tables)\n",
    "            used_tables_df = all_tables_df[all_tables_df['is_used'] == True].copy()\n",
    "            unused_tables_df = all_tables_df[all_tables_df['is_used'] == False].copy()\n",
    "        else:\n",
    "            used_tables_df = pd.DataFrame()\n",
    "            unused_tables_df = pd.DataFrame()\n",
    "        \n",
    "        # Filter measures\n",
    "        if not all_measures_df.empty:\n",
    "            # Create qualified measure name for comparison\n",
    "            all_measures_df['qualified_name'] = \"'\" + all_measures_df['table_name'] + \"'[\" + all_measures_df['measure_name'] + \"]\"\n",
    "            all_measures_df['is_used'] = all_measures_df['qualified_name'].isin(used_measures)\n",
    "            used_measures_df = all_measures_df[all_measures_df['is_used'] == True].copy()\n",
    "            unused_measures_df = all_measures_df[all_measures_df['is_used'] == False].copy()\n",
    "        else:\n",
    "            used_measures_df = pd.DataFrame()\n",
    "            unused_measures_df = pd.DataFrame()\n",
    "        \n",
    "        print(f\"  ‚úÖ Results filtered:\")\n",
    "        print(f\"    Used: {len(used_tables_df)} tables, {len(used_columns_df)} columns, {len(used_measures_df)} measures\")\n",
    "        print(f\"    Unused: {len(unused_tables_df)} tables, {len(unused_columns_df)} columns, {len(unused_measures_df)} measures\")\n",
    "        \n",
    "        return {\n",
    "            'used_tables': used_tables_df,\n",
    "            'used_columns': used_columns_df,\n",
    "            'used_measures': used_measures_df,\n",
    "            'unused_tables': unused_tables_df,\n",
    "            'unused_columns': unused_columns_df,\n",
    "            'unused_measures': unused_measures_df\n",
    "        }\n",
    "    \n",
    "    def collect_dataset_info(self, ds_id: str, ds_name: str, ws_id: str, ws_name: str) -> DatasetInfo:\n",
    "        \"\"\"Centralized function to collect all dataset-related information\"\"\"\n",
    "        dataset_info = DatasetInfo(ds_id, ds_name, ws_id, ws_name)\n",
    "        \n",
    "        # Get model dependencies\n",
    "        try:\n",
    "            deps = fabric.get_model_calc_dependencies(dataset=ds_id, workspace=ws_id)\n",
    "            with deps as calc_deps:\n",
    "                dependencies_df = getattr(calc_deps, \"dependencies_df\", None)\n",
    "            \n",
    "            if dependencies_df is not None and not dependencies_df.empty:\n",
    "                dependencies_df = self.sanitize_df_columns(\n",
    "                    df=dependencies_df, \n",
    "                    extra_columns=True,\n",
    "                    ws_id=ws_id, \n",
    "                    ds_id=ds_id,\n",
    "                    ws_name=ws_name,\n",
    "                    ds_name=ds_name\n",
    "                )\n",
    "                dataset_info.dependencies_df = dependencies_df\n",
    "            else:\n",
    "                dataset_info.dependencies_df = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            # Extract only the first line of the error message\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    ‚ö†Ô∏è Dependencies unavailable for {ds_name}: {brief_error}\")\n",
    "            # Store full error in error log\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'get_model_calc_dependencies',\n",
    "                'error': error_msg\n",
    "            })\n",
    "            dataset_info.dependencies_df = pd.DataFrame()\n",
    "        \n",
    "        #Get expressions\n",
    "        try:\n",
    "            expressions = fabric.list_expressions(dataset=ds_id, workspace=ws_id)\n",
    "            if not expressions.empty:\n",
    "                expressions = self.sanitize_df_columns(\n",
    "                    df=expressions,\n",
    "                    extra_columns=True,\n",
    "                    ws_id=ws_id,\n",
    "                    ds_id=ds_id,\n",
    "                    ws_name=ws_name,\n",
    "                    ds_name=ds_name\n",
    "                )\n",
    "            # Rename 'name' column to 'table_name'\n",
    "            if 'name' in expressions.columns:\n",
    "                expressions.rename(columns={'name': 'table_name'}, inplace=True)\n",
    "            \n",
    "            dataset_info.expressions_df = expressions\n",
    "            print(f\"    Found {len(expressions)} expressions\")\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    ‚ö†Ô∏è Expressions unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_expressions',\n",
    "                'error': error_msg\n",
    "            })\n",
    "            dataset_info.expressions_df = pd.DataFrame()\n",
    "\n",
    "        #Get Data Sources\n",
    "        try:\n",
    "            url = f\"https://api.powerbi.com/v1.0/myorg/datasets/{ds_id}/datasources\"\n",
    "\n",
    "            #Call API\n",
    "\n",
    "            response = fabric_client.get(url)\n",
    "\n",
    "            if response.ok:\n",
    "                datasource = response.json()\n",
    "                source_value = datasource['value']\n",
    "                if source_value or len(source_value) > 0:\n",
    "                    source_string = json.dumps(source_value)\n",
    "                    dataset_info.datasource = {\n",
    "                        'dataset_id': ds_id,\n",
    "                        'dataset_name': ds_name,\n",
    "                        'workspace_id': ws_id,\n",
    "                        'workspace_name': ws_name,\n",
    "                        'datasource': source_string\n",
    "                    }\n",
    "                    print(f\"    Found datsource for {ds_name}\")\n",
    "                else:\n",
    "                    dataset_info.datasource = None\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    ‚ö†Ô∏è Failed to get datasources for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'get datasources API Call',\n",
    "                'error': error_msg\n",
    "            })\n",
    "            dataset_info.datasource = {}\n",
    "        # Get tables\n",
    "        try:\n",
    "            tables = fabric.list_tables(dataset=ds_id, workspace=ws_id)\n",
    "            if not tables.empty:\n",
    "                tables = self.sanitize_df_columns(\n",
    "                    df=tables, \n",
    "                    extra_columns=True,\n",
    "                    ws_id=ws_id, \n",
    "                    ds_id=ds_id,\n",
    "                    ws_name=ws_name,\n",
    "                    ds_name=ds_name\n",
    "                )\n",
    "                dataset_info.tables_df = tables\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    ‚ö†Ô∏è Tables unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_tables',\n",
    "                'error': error_msg\n",
    "            })\n",
    "            \n",
    "        # Get relationships\n",
    "        try:\n",
    "            relationships = fabric.list_relationships(dataset=ds_id, workspace=ws_id, extended=True)\n",
    "            if not relationships.empty:\n",
    "                relationships = self.sanitize_df_columns(df=relationships)\n",
    "                relationships['qualified_from'] = \"'\" + relationships['from_table'] + \"'[\" + relationships['from_column'] + \"]\"\n",
    "                relationships['qualified_to'] = \"'\" + relationships['to_table'] + \"'[\" + relationships['to_column'] + \"]\"\n",
    "                dataset_info.relationships_df = relationships\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    ‚ö†Ô∏è Relationships unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_relationships',\n",
    "                'error': error_msg\n",
    "            })\n",
    "\n",
    "        # Get measures\n",
    "        try:\n",
    "            measures = fabric.list_measures(dataset=ds_id, workspace=ws_id)\n",
    "            if not measures.empty:\n",
    "                measures = self.sanitize_df_columns(df=measures)\n",
    "                dataset_info.measures_df = measures\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    ‚ö†Ô∏è Measures unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_measures',\n",
    "                'error': error_msg\n",
    "            })\n",
    "\n",
    "        # Get columns\n",
    "        try:\n",
    "            columns = fabric.list_columns(dataset=ds_id, workspace=ws_id, extended=True)\n",
    "            if not columns.empty:\n",
    "                columns = self.sanitize_df_columns(\n",
    "                    df=columns,\n",
    "                    extra_columns=True,\n",
    "                    ws_id=ws_id, \n",
    "                    ds_id=ds_id,\n",
    "                    ws_name=ws_name,\n",
    "                    ds_name=ds_name\n",
    "                )\n",
    "                columns['qualified_name'] = \"'\" + columns['table_name'] + \"'[\" + columns['column_name'] + ']'\n",
    "                dataset_info.columns_df = columns\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    ‚ö†Ô∏è Columns unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_columns',\n",
    "                'error': error_msg\n",
    "            })\n",
    "        \n",
    "        return dataset_info\n",
    "    \n",
    "    def extract_report_metadata(self, report_id: str, report_name: str, workspace_id: str, workspace_name: str, dataset_id: str) -> ReportMetadata:\n",
    "        \"\"\"Extract metadata from Power BI reports using dual approach\"\"\"\n",
    "        \n",
    "        # Initialize result object\n",
    "        result = ReportMetadata(\n",
    "            report_id=report_id,\n",
    "            report_name=report_name,\n",
    "            workspace_id=workspace_id,\n",
    "            workspace_name=workspace_name,\n",
    "            dataset_id=dataset_id,\n",
    "            report_format=\"Unknown\",\n",
    "            extraction_method=\"None\",\n",
    "            tables=[],\n",
    "            columns=[],\n",
    "            measures=[],\n",
    "            visuals_count=0,\n",
    "            filters_count=0,\n",
    "            extraction_success=False\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Try to determine report format\n",
    "            report = ReportWrapper(report=report_id, workspace=workspace_id)\n",
    "            rep_format = report.format\n",
    "            result.report_format = rep_format\n",
    "            print(f\"  üìë Report Type: {rep_format}\")\n",
    "            if rep_format == \"PBIR\":\n",
    "                # Method 1: Use sempy_labs.report.list_all_semantic_model_objects() for PBIR format\n",
    "                try:\n",
    "                    objects = report.list_semantic_model_objects()\n",
    "                    \n",
    "                    if objects is not None and not objects.empty:\n",
    "                        # Process the objects DataFrame\n",
    "                        tables = objects['Table Name'].unique().tolist()\n",
    "                        columns = (\n",
    "                            objects[objects['Object Type'] == 'Column']\n",
    "                            .assign(qualified=lambda df: \"'\" + df['Table Name'].fillna('') + \"'[\" + df['Object Name'] + \"]\")['qualified'] #build 'table'[column]\n",
    "                            .unique().tolist()\n",
    "                        )\n",
    "                        measures = (\n",
    "                            objects[objects['Object Type'] == 'Measure']\n",
    "                            .assign(qualified = lambda df: \"'\" + df['Table Name'].fillna('') + \"'[\" + df['Object Name'] + \"]\")[\"qualified\"] #build 'table'[measure]\n",
    "                            .unique().tolist()\n",
    "                        )\n",
    "                        \n",
    "                        result.tables = tables\n",
    "                        result.columns = columns\n",
    "                        result.measures = measures\n",
    "                        result.extraction_method = \"sempy_labs_objects\"\n",
    "                        result.extraction_success = True\n",
    "                        \n",
    "                        print(f\"    ‚úÖ Extracted via sempy_labs: {len(tables)} tables, {len(columns)} columns, {len(measures)} measures\")\n",
    "                        return result\n",
    "                        \n",
    "                except NotImplementedError as e:\n",
    "                    print(f\"    ‚ö†Ô∏è sempy_labs method not supported: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ö†Ô∏è sempy_labs method failed: {str(e)}\")\n",
    "            \n",
    "            # Method 2: Fall back to JSON parsing\n",
    "            report_json = sempy_labs.report.get_report_json(report=report_id, workspace=workspace_id)\n",
    "            \n",
    "            if report_json:\n",
    "                # Use our custom extractor\n",
    "                extractor = PowerBIMetadataExtractor()\n",
    "                extraction_results = extractor.extract_from_json_data(report_json)\n",
    "                \n",
    "                result.tables = extraction_results.get('tables', [])\n",
    "                result.columns = extraction_results.get('columns', [])\n",
    "                result.measures = extraction_results.get('measures', [])\n",
    "                result.visuals_count = len(extraction_results.get('visual_details', []))\n",
    "                result.filters_count = len(extraction_results.get('filter_details', []))\n",
    "                result.extraction_method = \"json_parsing\"\n",
    "                result.extraction_success = True\n",
    "                \n",
    "                print(f\"    ‚úÖ Extracted via JSON: {len(result.tables)} tables, {len(result.columns)} columns, {len(result.measures)} measures\")\n",
    "                return result\n",
    "            else:\n",
    "                result.error_message = \"Could not retrieve report JSON\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            result.error_message = f\"Extraction failed: {str(e)}\"\n",
    "            print(f\"    ‚ùå Error extracting metadata: {str(e)}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_ai_dataset_context(\n",
    "        self, \n",
    "        all_tables_df, \n",
    "        all_columns_df, \n",
    "        all_measures_df, \n",
    "        all_relationships_df, \n",
    "        all_datasources_df,\n",
    "        filtered_results\n",
    "    ):\n",
    "        \"\"\"Generate AI-optimized dataset context table with health scores\"\"\"\n",
    "        print(\"\\nü§ñ Generating AI dataset context table...\")\n",
    "        \n",
    "        ai_dataset_records = []\n",
    "        \n",
    "        for ds_id, dataset_info in self.all_dataset_info.items():\n",
    "            # Get basic dataset info\n",
    "            ds_row = self.datasets_df[self.datasets_df['dataset_id'] == ds_id].iloc[0] if not self.datasets_df[self.datasets_df['dataset_id'] == ds_id].empty else None\n",
    "            if ds_row is None:\n",
    "                continue\n",
    "            \n",
    "            # Count objects for this dataset\n",
    "            total_tables = len(all_tables_df[all_tables_df['dataset_id'] == ds_id]) if not all_tables_df.empty else 0\n",
    "            total_columns = len(all_columns_df[all_columns_df['dataset_id'] == ds_id]) if not all_columns_df.empty else 0\n",
    "            total_measures = len(all_measures_df[all_measures_df['dataset_id'] == ds_id]) if not all_measures_df.empty else 0\n",
    "            total_relationships = len(all_relationships_df[all_relationships_df['dataset_id'] == ds_id]) if not all_relationships_df.empty else 0\n",
    "            \n",
    "            # Count report usage\n",
    "            report_count = len(self.reports_df[self.reports_df['dataset_id'] == ds_id]) if not self.reports_df.empty else 0\n",
    "            \n",
    "            # Count connected vs isolated tables\n",
    "            dataset_tables = all_tables_df[all_tables_df['dataset_id'] == ds_id]['name'].tolist() if not all_tables_df.empty else []\n",
    "            connected_tables_set = set()\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                connected_tables_set.update(dataset_info.relationships_df['from_table'].tolist())\n",
    "                connected_tables_set.update(dataset_info.relationships_df['to_table'].tolist())\n",
    "            connected_tables = len(connected_tables_set)\n",
    "            isolated_tables = total_tables - connected_tables\n",
    "            \n",
    "            # Count unused objects\n",
    "            unused_tables = len(filtered_results['unused_tables'][filtered_results['unused_tables']['dataset_id'] == ds_id]) if not filtered_results['unused_tables'].empty else 0\n",
    "            unused_columns = len(filtered_results['unused_columns'][filtered_results['unused_columns']['dataset_id'] == ds_id]) if not filtered_results['unused_columns'].empty else 0\n",
    "            unused_measures = len(filtered_results['unused_measures'][filtered_results['unused_measures']['dataset_id'] == ds_id]) if not filtered_results['unused_measures'].empty else 0\n",
    "            \n",
    "            # Detect circular relationships (simplified - count self-referencing)\n",
    "            circular_chains = 0\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                # Simple circular detection: tables that reference themselves\n",
    "                circular_chains = len(dataset_info.relationships_df[\n",
    "                    dataset_info.relationships_df['from_table'] == dataset_info.relationships_df['to_table']\n",
    "                ])\n",
    "            \n",
    "            # Calculate health scores (0-1 scale)\n",
    "            relationship_health = (connected_tables / total_tables) if total_tables > 0 else 0.0\n",
    "            usage_efficiency = 1 - (unused_columns / total_columns) if total_columns > 0 else 1.0\n",
    "            \n",
    "            # Model complexity score (normalized)\n",
    "            # Higher complexity = more relationships + measures relative to tables\n",
    "            complexity_raw = (total_relationships + total_measures) / max(total_tables, 1)\n",
    "            model_complexity = min(complexity_raw / 10, 1.0)  # Normalize to 0-1, cap at 1\n",
    "            \n",
    "            # Overall optimization score (0-100)\n",
    "            # Weighted combination: relationships (30%), usage (40%), no isolated tables (20%), no unused (10%)\n",
    "            optimization_score = (\n",
    "                relationship_health * 30 +\n",
    "                usage_efficiency * 40 +\n",
    "                ((total_tables - isolated_tables) / max(total_tables, 1)) * 20 +\n",
    "                (1 - (unused_tables / max(total_tables, 1))) * 10\n",
    "            )\n",
    "\n",
    "            # Extract datasource as scalar value, not Series\n",
    "            datasource_series = all_datasources_df[all_datasources_df['dataset_id'] == ds_id]['datasource']\n",
    "            datasource = datasource_series.iloc[0] if not datasource_series.empty else None\n",
    "            \n",
    "            ai_dataset_records.append({\n",
    "                'workspace_id': dataset_info.ws_id,\n",
    "                'workspace_name': dataset_info.ws_name,\n",
    "                'dataset_id': ds_id,\n",
    "                'dataset_name': dataset_info.ds_name,\n",
    "                'datasource': datasource,\n",
    "                # Size metrics\n",
    "                'total_tables': total_tables,\n",
    "                'total_columns': total_columns,\n",
    "                'total_measures': total_measures,\n",
    "                'total_relationships': total_relationships,\n",
    "                # Usage metrics\n",
    "                'report_count': report_count,\n",
    "                'dataflow_count': 0,  # Placeholder - can be added if needed\n",
    "                'connected_tables': connected_tables,\n",
    "                'isolated_tables': isolated_tables,\n",
    "                # Quality metrics\n",
    "                'unused_tables': unused_tables,\n",
    "                'unused_columns': unused_columns,\n",
    "                'unused_measures': unused_measures,\n",
    "                'circular_chains': circular_chains,\n",
    "                # Calculated health scores\n",
    "                'relationship_health': round(relationship_health, 3),\n",
    "                'usage_efficiency': round(usage_efficiency, 3),\n",
    "                'model_complexity': round(model_complexity, 3),\n",
    "                'optimization_score': round(optimization_score, 2)\n",
    "            })\n",
    "        \n",
    "        ai_dataset_context_df = pd.DataFrame(ai_dataset_records)\n",
    "        print(f\"  ‚úÖ Generated {len(ai_dataset_records)} dataset context records\")\n",
    "        return ai_dataset_context_df\n",
    "    \n",
    "    def generate_ai_object_features(\n",
    "        self, \n",
    "        all_columns_df, \n",
    "        all_measures_df, \n",
    "        all_tables_df, \n",
    "        all_relationships_df, \n",
    "        filtered_results, \n",
    "        ai_dataset_context_df\n",
    "    ):\n",
    "        \"\"\"Generate AI-optimized object features table with rich context\"\"\"\n",
    "        print(\"\\nü§ñ Generating AI object features table...\")\n",
    "        \n",
    "        ai_object_records = []\n",
    "        \n",
    "        # Process columns\n",
    "        if not all_columns_df.empty:\n",
    "            for _, col_row in all_columns_df.iterrows():\n",
    "                ds_id = col_row.get('dataset_id', '')\n",
    "                table_name = col_row.get('table_name', '')\n",
    "                column_name = col_row.get('column_name', '')\n",
    "                qualified_name = col_row.get('qualified_name', '')\n",
    "                \n",
    "                # Get dataset context\n",
    "                dataset_context = ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].iloc[0] if not ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].empty else None\n",
    "                \n",
    "                # Get table context\n",
    "                table_measures = len(all_measures_df[\n",
    "                    (all_measures_df['dataset_id'] == ds_id) & \n",
    "                    (all_measures_df['table_name'] == table_name)\n",
    "                ]) if not all_measures_df.empty else 0\n",
    "                \n",
    "                table_columns = len(all_columns_df[\n",
    "                    (all_columns_df['dataset_id'] == ds_id) & \n",
    "                    (all_columns_df['table_name'] == table_name)\n",
    "                ]) if not all_columns_df.empty else 0\n",
    "                \n",
    "                table_relationships = len(all_relationships_df[\n",
    "                    (all_relationships_df['dataset_id'] == ds_id) & \n",
    "                    ((all_relationships_df['from_table'] == table_name) | \n",
    "                     (all_relationships_df['to_table'] == table_name))\n",
    "                ]) if not all_relationships_df.empty else 0\n",
    "                \n",
    "                # Check if table is isolated\n",
    "                table_is_isolated = table_relationships == 0\n",
    "                \n",
    "                # Check column usage\n",
    "                is_used = col_row.get('is_used', False)\n",
    "                \n",
    "                # Count usage types from dependencies\n",
    "                dataset_info = self.all_dataset_info.get(ds_id)\n",
    "                used_by_measures = 0\n",
    "                used_by_relationships = 0\n",
    "                used_by_dependencies = 0\n",
    "                referenced_by_list = []\n",
    "                \n",
    "                if dataset_info and dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
    "                    deps = dataset_info.dependencies_df[\n",
    "                        dataset_info.dependencies_df['referenced_full_object_name'] == qualified_name\n",
    "                    ]\n",
    "                    used_by_dependencies = len(deps)\n",
    "                    \n",
    "                    # Get specific usage types\n",
    "                    if 'object_type' in deps.columns:\n",
    "                        used_by_measures = len(deps[deps['object_type'] == 'Measure'])\n",
    "                    \n",
    "                    # Get referenced by list\n",
    "                    if 'object_name' in deps.columns:\n",
    "                        referenced_by_list = deps['object_name'].unique().tolist()\n",
    "                \n",
    "                # Check relationship usage\n",
    "                if not all_relationships_df.empty:\n",
    "                    rel_usage = all_relationships_df[\n",
    "                        (all_relationships_df['dataset_id'] == ds_id) & \n",
    "                        ((all_relationships_df['qualified_from'] == qualified_name) | \n",
    "                         (all_relationships_df['qualified_to'] == qualified_name))\n",
    "                    ]\n",
    "                    used_by_relationships = len(rel_usage)\n",
    "                \n",
    "                # Calculate usage score (0-1)\n",
    "                usage_score = min((used_by_measures * 0.4 + used_by_relationships * 0.4 + used_by_dependencies * 0.2) / 5, 1.0)\n",
    "                \n",
    "                ai_object_records.append({\n",
    "                    'workspace_id': col_row.get('workspace_id', ''),\n",
    "                    'workspace_name': col_row.get('workspace_name', ''),\n",
    "                    'dataset_id': ds_id,\n",
    "                    'dataset_name': col_row.get('dataset_name', ''),\n",
    "                    'table_name': table_name,\n",
    "                    'object_name': column_name,\n",
    "                    'object_type': 'calculated_column' if col_row.get('type', '').lower() == 'calculated' else 'column',\n",
    "                    # Object properties\n",
    "                    'data_type': col_row.get('data_type', col_row.get('type', 'Unknown')),\n",
    "                    'is_hidden': col_row.get('is_hidden', False),\n",
    "                    'is_calculated': col_row.get('type', '').lower() == 'calculated',\n",
    "                    'has_dax': col_row.get('expression', '') != '',\n",
    "                    # Table context\n",
    "                    'table_measure_count': table_measures,\n",
    "                    'table_column_count': table_columns,\n",
    "                    'table_relationship_count': table_relationships,\n",
    "                    'table_is_isolated': table_is_isolated,\n",
    "                    # Dataset context (denormalized)\n",
    "                    'dataset_total_tables': dataset_context['total_tables'] if dataset_context is not None else 0,\n",
    "                    'dataset_relationship_health': dataset_context['relationship_health'] if dataset_context is not None else 0.0,\n",
    "                    'dataset_usage_efficiency': dataset_context['usage_efficiency'] if dataset_context is not None else 0.0,\n",
    "                    # Usage features\n",
    "                    'used_by_measures': used_by_measures,\n",
    "                    'used_by_relationships': used_by_relationships,\n",
    "                    'used_by_dependencies': used_by_dependencies,\n",
    "                    'is_used': is_used,\n",
    "                    'usage_score': round(usage_score, 3),\n",
    "                    # Referenced by (as JSON string)\n",
    "                    'referenced_by_list': json.dumps(referenced_by_list) if referenced_by_list else ''\n",
    "                })\n",
    "        \n",
    "        # Process measures\n",
    "        if not all_measures_df.empty:\n",
    "            for _, meas_row in all_measures_df.iterrows():\n",
    "                ds_id = meas_row.get('dataset_id', '')\n",
    "                table_name = meas_row.get('table_name', '')\n",
    "                measure_name = meas_row.get('measure_name', '')\n",
    "                qualified_name = meas_row.get('qualified_name', '')\n",
    "                \n",
    "                # Get dataset context\n",
    "                dataset_context = ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].iloc[0] if not ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].empty else None\n",
    "                \n",
    "                # Get table context\n",
    "                table_measures = len(all_measures_df[\n",
    "                    (all_measures_df['dataset_id'] == ds_id) & \n",
    "                    (all_measures_df['table_name'] == table_name)\n",
    "                ]) if not all_measures_df.empty else 0\n",
    "                \n",
    "                table_columns = len(all_columns_df[\n",
    "                    (all_columns_df['dataset_id'] == ds_id) & \n",
    "                    (all_columns_df['table_name'] == table_name)\n",
    "                ]) if not all_columns_df.empty else 0\n",
    "                \n",
    "                table_relationships = len(all_relationships_df[\n",
    "                    (all_relationships_df['dataset_id'] == ds_id) & \n",
    "                    ((all_relationships_df['from_table'] == table_name) | \n",
    "                     (all_relationships_df['to_table'] == table_name))\n",
    "                ]) if not all_relationships_df.empty else 0\n",
    "                \n",
    "                table_is_isolated = table_relationships == 0\n",
    "                \n",
    "                # Check measure usage\n",
    "                is_used = meas_row.get('is_used', False)\n",
    "                \n",
    "                # Count usage from dependencies\n",
    "                dataset_info = self.all_dataset_info.get(ds_id)\n",
    "                used_by_measures = 0\n",
    "                used_by_dependencies = 0\n",
    "                referenced_by_list = []\n",
    "                \n",
    "                if dataset_info and dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
    "                    deps = dataset_info.dependencies_df[\n",
    "                        dataset_info.dependencies_df['referenced_full_object_name'] == qualified_name\n",
    "                    ]\n",
    "                    used_by_dependencies = len(deps)\n",
    "                    \n",
    "                    if 'object_type' in deps.columns:\n",
    "                        used_by_measures = len(deps[deps['object_type'] == 'Measure'])\n",
    "                    \n",
    "                    if 'object_name' in deps.columns:\n",
    "                        referenced_by_list = deps['object_name'].unique().tolist()\n",
    "                \n",
    "                # Calculate usage score\n",
    "                usage_score = min((used_by_measures * 0.5 + used_by_dependencies * 0.5) / 3, 1.0)\n",
    "                \n",
    "                ai_object_records.append({\n",
    "                    'workspace_id': meas_row.get('workspace_id', ''),\n",
    "                    'workspace_name': meas_row.get('workspace_name', ''),\n",
    "                    'dataset_id': ds_id,\n",
    "                    'dataset_name': meas_row.get('dataset_name', ''),\n",
    "                    'table_name': table_name,\n",
    "                    'object_name': measure_name,\n",
    "                    'object_type': 'measure',\n",
    "                    # Object properties\n",
    "                    'data_type': 'Measure',\n",
    "                    'is_hidden': meas_row.get('is_hidden', False),\n",
    "                    'is_calculated': True,\n",
    "                    'has_dax': meas_row.get('expression', '') != '',\n",
    "                    # Table context\n",
    "                    'table_measure_count': table_measures,\n",
    "                    'table_column_count': table_columns,\n",
    "                    'table_relationship_count': table_relationships,\n",
    "                    'table_is_isolated': table_is_isolated,\n",
    "                    # Dataset context\n",
    "                    'dataset_total_tables': dataset_context['total_tables'] if dataset_context is not None else 0,\n",
    "                    'dataset_relationship_health': dataset_context['relationship_health'] if dataset_context is not None else 0.0,\n",
    "                    'dataset_usage_efficiency': dataset_context['usage_efficiency'] if dataset_context is not None else 0.0,\n",
    "                    # Usage features\n",
    "                    'used_by_measures': used_by_measures,\n",
    "                    'used_by_relationships': 0,  # Measures aren't used in relationships\n",
    "                    'used_by_dependencies': used_by_dependencies,\n",
    "                    'is_used': is_used,\n",
    "                    'usage_score': round(usage_score, 3),\n",
    "                    # Referenced by\n",
    "                    'referenced_by_list': json.dumps(referenced_by_list) if referenced_by_list else ''\n",
    "                })\n",
    "        \n",
    "        ai_object_features_df = pd.DataFrame(ai_object_records)\n",
    "        print(f\"  ‚úÖ Generated {len(ai_object_records)} object feature records\")\n",
    "        return ai_object_features_df\n",
    "    \n",
    "    def normalize_datasources(self, all_datasources_df):\n",
    "        \"\"\"\n",
    "        Normalize datasource information into a structured table.\n",
    "        Parses JSON datasource strings and extracts connection details based on datasource type.\n",
    "        \n",
    "        Reference: https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-datasources\n",
    "        \"\"\"\n",
    "        print(\"\\nüîå Normalizing datasource information...\")\n",
    "        \n",
    "        normalized_records = []\n",
    "        \n",
    "        if all_datasources_df.empty:\n",
    "            print(\"  ‚ö†Ô∏è  No datasources to normalize\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        for _, ds_row in all_datasources_df.iterrows():\n",
    "            dataset_id = ds_row.get('dataset_id', '')\n",
    "            dataset_name = ds_row.get('dataset_name', '')\n",
    "            workspace_id = ds_row.get('workspace_id', '')\n",
    "            workspace_name = ds_row.get('workspace_name', '')\n",
    "            datasource_json_str = ds_row.get('datasource', '')\n",
    "            \n",
    "            if not datasource_json_str:\n",
    "                # No datasource for this dataset\n",
    "                normalized_records.append({\n",
    "                    'dataset_id': dataset_id,\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'workspace_id': workspace_id,\n",
    "                    'workspace_name': workspace_name,\n",
    "                    'has_datasource': False,\n",
    "                    'datasource_type': None,\n",
    "                    'server': None,\n",
    "                    'source': None,\n",
    "                    'datasource_id': None,\n",
    "                    'gateway_id': None\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Parse JSON string\n",
    "                datasources = json.loads(datasource_json_str)\n",
    "                \n",
    "                # Handle case where datasources is a single object instead of array\n",
    "                if not isinstance(datasources, list):\n",
    "                    datasources = [datasources]\n",
    "                \n",
    "                # Process each datasource (a dataset can have multiple)\n",
    "                for ds in datasources:\n",
    "                    datasource_type = ds.get('datasourceType', 'Unknown')\n",
    "                    connection_details = ds.get('connectionDetails', {})\n",
    "                    datasource_id = ds.get('datasourceId', '')\n",
    "                    gateway_id = ds.get('gatewayId', '')\n",
    "                    \n",
    "                    # Extract server and source based on datasource type\n",
    "                    # Reference: https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-datasources\n",
    "                    server = None\n",
    "                    source = None\n",
    "                    \n",
    "                    if datasource_type in ['Sql', 'Oracle', 'SAPHana', 'AnalysisServices']:\n",
    "                        # For SQL types, extract server separately\n",
    "                        server = connection_details.get('server')\n",
    "                        source = connection_details.get('database')\n",
    "                    elif datasource_type == 'Salesforce':\n",
    "                        # Salesforce: loginServer is server-like, classInfo is source\n",
    "                        server = connection_details.get('loginServer')\n",
    "                        source = connection_details.get('classInfo')\n",
    "                    elif datasource_type == 'File':\n",
    "                        source = connection_details.get('path')\n",
    "                    elif datasource_type in ['Web', 'SharePointList', 'OData']:\n",
    "                        source = connection_details.get('url')\n",
    "                    elif datasource_type == 'AzureBlobs':\n",
    "                        # AzureBlobs: account and domain together form the endpoint\n",
    "                        # Keep combined in source as it represents the full connection endpoint\n",
    "                        account = connection_details.get('account')\n",
    "                        domain = connection_details.get('domain')\n",
    "                        if account and domain:\n",
    "                            source = f\"{account}.{domain}\"\n",
    "                        elif account:\n",
    "                            source = account\n",
    "                        elif domain:\n",
    "                            source = domain\n",
    "                    elif datasource_type == 'Exchange':\n",
    "                        source = connection_details.get('emailAddress')\n",
    "                    elif datasource_type == 'Extension':\n",
    "                        # Extension: kind is the extension type, path is the connection endpoint\n",
    "                        # Keep path in source as it's the actual connection point\n",
    "                        path = connection_details.get('path')\n",
    "                        kind = connection_details.get('kind')\n",
    "                        if path:\n",
    "                            source = path\n",
    "                        elif kind:\n",
    "                            source = kind\n",
    "                    \n",
    "                    normalized_records.append({\n",
    "                        'dataset_id': dataset_id,\n",
    "                        'dataset_name': dataset_name,\n",
    "                        'workspace_id': workspace_id,\n",
    "                        'workspace_name': workspace_name,\n",
    "                        'has_datasource': True,\n",
    "                        'datasource_type': datasource_type,\n",
    "                        'server': server,\n",
    "                        'source': source,\n",
    "                        'datasource_id': datasource_id,\n",
    "                        'gateway_id': gateway_id\n",
    "                    })\n",
    "            \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error parsing datasource JSON for {dataset_name}: {e}\")\n",
    "                # Add record with error indication\n",
    "                normalized_records.append({\n",
    "                    'dataset_id': dataset_id,\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'workspace_id': workspace_id,\n",
    "                    'workspace_name': workspace_name,\n",
    "                    'has_datasource': False,\n",
    "                    'datasource_type': 'ERROR',\n",
    "                    'server': None,\n",
    "                    'source': None,\n",
    "                    'datasource_id': None,\n",
    "                    'gateway_id': None\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Unexpected error processing datasource for {dataset_name}: {e}\")\n",
    "        \n",
    "        normalized_df = pd.DataFrame(normalized_records)\n",
    "        print(f\"  ‚úÖ Normalized {len(normalized_records)} datasource records\")\n",
    "        \n",
    "        # Print summary by datasource type\n",
    "        if not normalized_df.empty:\n",
    "            type_counts = normalized_df[normalized_df['has_datasource'] == True]['datasource_type'].value_counts()\n",
    "            if not type_counts.empty:\n",
    "                print(f\"  üìä Datasource types found:\")\n",
    "                for ds_type, count in type_counts.items():\n",
    "                    print(f\"     - {ds_type}: {count}\")\n",
    "        \n",
    "        return normalized_df\n",
    "    \n",
    "    def save_all_results(\n",
    "            self, \n",
    "            all_columns_df, \n",
    "            all_tables_df, \n",
    "            all_measures_df, \n",
    "            all_dependencies_df, \n",
    "            all_relationships_df, \n",
    "            filtered_results, \n",
    "            dependencies, \n",
    "            all_expressions_df, \n",
    "            all_datasources_df\n",
    "        ):\n",
    "        \"\"\"Save AI-optimized results to lakehouse\"\"\"\n",
    "        print(\"\\nüíæ STEP 7: Saving AI-optimized results to lakehouse...\")\n",
    "        \n",
    "        # Generate AI-optimized tables\n",
    "        ai_dataset_context_df = self.generate_ai_dataset_context(\n",
    "            all_tables_df, all_columns_df, all_measures_df, all_relationships_df, all_datasources_df, filtered_results\n",
    "        )\n",
    "        \n",
    "        ai_object_features_df = self.generate_ai_object_features(\n",
    "            all_columns_df, all_measures_df, all_tables_df, all_relationships_df, filtered_results, ai_dataset_context_df\n",
    "        )\n",
    "        \n",
    "        # Save ONLY AI-optimized tables for ML/AI consumption\n",
    "        print(\"\\nü§ñ Saving AI-optimized tables for ML training and predictions...\")\n",
    "        self.save_to_lakehouse(ai_dataset_context_df, \"ai_dataset_context\", \n",
    "                              \"AI-ready dataset features with health scores (0-100) for ML training\")\n",
    "        self.save_to_lakehouse(ai_object_features_df, \"ai_object_features\", \n",
    "                              \"AI-ready object-level features with full lineage context for predictions\")\n",
    "        \n",
    "        print(\"\\nüîó Saving relationships table...\")\n",
    "        self.save_to_lakehouse(all_relationships_df, \"dataset_relationships\", \n",
    "                              \"All relationships across datasets with qualified column references\")\n",
    "\n",
    "        print(\"\\nüîó Saving dependencies table...\")\n",
    "        self.save_to_lakehouse(all_dependencies_df, \"dataste_dependencies\",\n",
    "                                \"All dataset dependedncies for reference later\")\n",
    "\n",
    "        print(\"\\nüìù Saving expressions table...\")\n",
    "        self.save_to_lakehouse(all_expressions_df, \"dataset_expressions\", \n",
    "                          \"M code expressions from Power Query for all datasets\")\n",
    "        \n",
    "        # Normalize and save datasources\n",
    "        print(\"\\nüîå Normalizing and saving datasources table...\")\n",
    "        normalized_datasources_df = self.normalize_datasources(all_datasources_df)\n",
    "        self.save_to_lakehouse(normalized_datasources_df, \"dataset_datasources\", \n",
    "                              \"Normalized datasource information with connection details by type\")\n",
    "\n",
    "        print(\"\\n‚úÖ AI-optimized lakehouse tables created successfully!\")\n",
    "        print(f\"   üìä ai_dataset_context: {len(ai_dataset_context_df)} datasets with 16 features\")\n",
    "        print(f\"   üìä ai_object_features: {len(ai_object_features_df)} objects (columns + measures) with 23 features\")\n",
    "        print(\"\\nüí° Use these tables for:\")\n",
    "        print(\"   - Training schema optimization models\")\n",
    "        print(\"   - Predicting unused objects\")\n",
    "        print(\"   - Generating dataset health scores\")\n",
    "        print(\"   - Recommending model improvements\")\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run the complete analysis workflow\"\"\"\n",
    "        print(\"üöÄ STARTING COMPLETE FABRIC WORKSPACE ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Get Workspaces\n",
    "        self.get_workspaces()\n",
    "        \n",
    "        # Step 2: Get Datasets and Reports\n",
    "        self.get_datasets_and_reports()\n",
    "        \n",
    "        # Step 3: Process all datasets and aggregate all objects\n",
    "        (all_columns_df, \n",
    "        all_tables_df, \n",
    "        all_measures_df, \n",
    "        all_dependencies_df, \n",
    "        all_relationships_df, \n",
    "        all_expressions_df, \n",
    "        all_datasources_df) = self.process_all_datasets()\n",
    "        \n",
    "        # Step 4: Get report metadata\n",
    "        self.get_reports_metadata()\n",
    "        \n",
    "        # Step 5: Check dependencies\n",
    "        dependencies = self.check_dependencies(all_columns_df, all_tables_df, all_measures_df)\n",
    "        \n",
    "        # Step 6: Filter results\n",
    "        filtered_results = self.filter_results(all_columns_df, all_tables_df, all_measures_df, dependencies)\n",
    "        \n",
    "        # Step 7: Save all results\n",
    "        self.save_all_results(\n",
    "            all_columns_df, \n",
    "            all_tables_df, \n",
    "            all_measures_df, \n",
    "            all_dependencies_df, \n",
    "            all_relationships_df, \n",
    "            filtered_results, \n",
    "            dependencies, \n",
    "            all_expressions_df, \n",
    "            all_datasources_df\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üéâ FABRIC WORKSPACE ANALYSIS COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"‚è±Ô∏è Total execution time: {duration:.2f} seconds\")\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"  Workspaces analyzed: {len(self.workspaces_df)}\")\n",
    "        print(f\"  Datasets processed: {len(self.datasets_df)}\")\n",
    "        print(f\"  Reports analyzed: {len(self.report_metadata_list)}\")\n",
    "        print(f\"  Total objects found: {len(all_columns_df)} columns, {len(all_tables_df)} tables, {len(all_measures_df)} measures\")\n",
    "        print(f\"  Used objects: {len(filtered_results['used_columns'])} columns, {len(filtered_results['used_tables'])} tables, {len(filtered_results['used_measures'])} measures\")\n",
    "        print(f\"  Unused objects: {len(filtered_results['unused_columns'])} columns, {len(filtered_results['unused_tables'])} tables, {len(filtered_results['unused_measures'])} measures\")\n",
    "        print(\"\\nüíæ All results saved to lakehouse tables!\")\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97078286-a3aa-42c3-a224-5f90f955b02c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-11-04T07:45:23.4355758Z",
       "execution_start_time": "2025-11-04T07:41:24.2645127Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2a002675-dd15-4654-906f-5a46bf5a72e6",
       "queued_time": "2025-11-04T07:41:24.2634739Z",
       "session_id": "78e0656b-e32a-4987-98b4-d04ec9c9a998",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 27,
       "statement_ids": [
        27
       ]
      },
      "text/plain": [
       "StatementMeta(, 78e0656b-e32a-4987-98b4-d04ec9c9a998, 27, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Fabric Workspace Analyzer...\n",
      "\n",
      "üöÄ STARTING COMPLETE FABRIC WORKSPACE ANALYSIS\n",
      "================================================================================\n",
      "üîç STEP 1: Discovering workspaces...\n",
      "  ‚úÖ Found 4 workspaces\n",
      "\n",
      "üîç STEP 2: Getting datasets and reports...\n",
      "  üì¶ Scanning workspace: Fabric_Demo\n",
      "  üì¶ Scanning workspace: BI_Metadata\n",
      "  üì¶ Scanning workspace: NLQ_Task\n",
      "  üì¶ Scanning workspace: Auto_DP\n",
      "  ‚úÖ Found 8 datasets and 11 reports (7 PowerBI reports)\n",
      "\n",
      "üîç STEP 3: Processing all datasets and aggregating objects...\n",
      "  üìä Processing dataset: LH_D365FNO\n",
      "    ‚ö†Ô∏è Dependencies unavailable for LH_D365FNO: The database is empty. The DISCOVER_CALC_DEPENDENCY operation cannot be performed on an empty database.\n",
      "    Found 0 expressions\n",
      "  üìä Processing dataset: NLQ_Model\n",
      "    Found 1 expressions\n",
      "    Found datsource for NLQ_Model\n",
      "  üìä Processing dataset: NLQ_Demo\n",
      "    Found 1 expressions\n",
      "    Found datsource for NLQ_Demo\n",
      "  üìä Processing dataset: IT Spend Analysis Sample PBIX\n",
      "    Found 0 expressions\n",
      "    Found datsource for IT Spend Analysis Sample PBIX\n",
      "  üìä Processing dataset: COVID Bakeoff\n",
      "    Found 8 expressions\n",
      "    Found datsource for COVID Bakeoff\n",
      "  üìä Processing dataset: Sales & Returns Sample v201912\n",
      "    Found 0 expressions\n",
      "    Found datsource for Sales & Returns Sample v201912\n",
      "  üìä Processing dataset: Sales & Returns Sample PBIX\n",
      "    Found 0 expressions\n",
      "    Found datsource for Sales & Returns Sample PBIX\n",
      "  üìä Processing dataset: COVID Bakeoff PBIR\n",
      "    Found 8 expressions\n",
      "    Found datsource for COVID Bakeoff PBIR\n",
      "  ‚úÖ Processed 8 datasets\n",
      "    üìã Aggregated: 732 columns, 71 tables, 186 measures\n",
      "    üîó Aggregated: 3148 dependencies, 58 relationships, 18 Expressions\n",
      "\n",
      "üîç STEP 4: Extracting report metadata...\n",
      "  üìä Processing report 1/8: NLQ\n",
      "  üìë Report Type: PBIR-Legacy\n",
      "    ‚úÖ Extracted via JSON: 0 tables, 0 columns, 0 measures\n",
      "  üìä Processing report 2/8: NLQ_Demo\n",
      "  üìë Report Type: PBIR-Legacy\n",
      "    ‚úÖ Extracted via JSON: 2 tables, 4 columns, 1 measures\n",
      "  üìä Processing report 3/8: IT Spend Analysis Sample PBIX\n",
      "  üìë Report Type: PBIR-Legacy\n",
      "    ‚úÖ Extracted via JSON: 8 tables, 11 columns, 5 measures\n",
      "  üìä Processing report 4/8: COVID Bakeoff\n",
      "  üìë Report Type: PBIR-Legacy\n",
      "    ‚úÖ Extracted via JSON: 11 tables, 24 columns, 15 measures\n",
      "  üìä Processing report 5/8: Sales & Returns Sample v201912\n",
      "  üìë Report Type: PBIR\n",
      "    ‚úÖ Extracted via sempy_labs: 14 tables, 42 columns, 24 measures\n",
      "  üìä Processing report 6/8: Sales & Returns Sample PBIX\n",
      "  üìë Report Type: PBIR-Legacy\n",
      "    ‚úÖ Extracted via JSON: 22 tables, 37 columns, 21 measures\n",
      "  üìä Processing report 7/8: COVID Bakeoff PBIR\n",
      "  üìë Report Type: PBIR-Legacy\n",
      "  ‚úÖ Saved 918 records to 'ai_object_features' table\n",
      "     üìù AI-ready object-level features with full lineage context for predictions\n",
      "\n",
      "üîó Saving relationships table...\n",
      "  ‚úÖ Saved 58 records to 'dataset_relationships' table\n",
      "     üìù All relationships across datasets with qualified column references\n",
      "\n",
      "üîó Saving dependencies table...\n",
      "  ‚úÖ Saved 3148 records to 'dataste_dependencies' table\n",
      "     üìù All dataset dependedncies for reference later\n",
      "\n",
      "üìù Saving expressions table...\n",
      "  ‚úÖ Saved 18 records to 'dataset_expressions' table\n",
      "     üìù M code expressions from Power Query for all datasets\n",
      "\n",
      "‚úÖ AI-optimized lakehouse tables created successfully!\n",
      "   üìä ai_dataset_context: 8 datasets with 16 features\n",
      "   üìä ai_object_features: 918 objects (columns + measures) with 23 features\n",
      "\n",
      "üí° Use these tables for:\n",
      "   - Training schema optimization models\n",
      "   - Predicting unused objects\n",
      "   - Generating dataset health scores\n",
      "   - Recommending model improvements\n",
      "\n",
      "================================================================================\n",
      "üéâ FABRIC WORKSPACE ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "‚è±Ô∏è Total execution time: 233.94 seconds\n",
      "\n",
      "üìä Summary:\n",
      "  Workspaces analyzed: 4\n",
      "  Datasets processed: 8\n",
      "  Reports analyzed: 7\n",
      "  Total objects found: 732 columns, 71 tables, 186 measures\n",
      "  Used objects: 237 columns, 54 tables, 102 measures\n",
      "  Unused objects: 495 columns, 17 tables, 84 measures\n",
      "\n",
      "üíæ All results saved to lakehouse tables!\n",
      "================================================================================\n",
      "\n",
      "üéâ Analysis complete! AI-optimized tables are ready.\n",
      "\n",
      "üìä Quick Access:\n",
      "  ai_datasets = spark.table('ai_dataset_context').toPandas()\n",
      "  ai_objects = spark.table('ai_object_features').toPandas()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Could not convert Series([], Name: datasource, dtype: object) with type Series: did not recognize Python value type when inferring an Arrow data type\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# RUN THE COMPLETE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üöÄ Initializing Fabric Workspace Analyzer...\\n\")\n",
    "analyzer = FabricWorkspaceAnalyzer()\n",
    "\n",
    "# Run the complete analysis\n",
    "analyzer.run_complete_analysis()\n",
    "\n",
    "print(\"\\nüéâ Analysis complete! AI-optimized tables are ready.\")\n",
    "print(\"\\nüìä Quick Access:\")\n",
    "print(\"  ai_datasets = spark.table('ai_dataset_context').toPandas()\")\n",
    "print(\"  ai_objects = spark.table('ai_object_features').toPandas()\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "6f0b7983-bd9a-43fa-8269-8e327ed1e996",
    "default_lakehouse_name": "Migration_LH",
    "default_lakehouse_workspace_id": "928ee79e-5cc1-44b3-8d63-89320ca539c9",
    "known_lakehouses": [
     {
      "id": "6f0b7983-bd9a-43fa-8269-8e327ed1e996"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
